# Data-Bootcamp-Final-Project
Using models to predict NBA All-Star Game selections. 
  For this final project, I wanted to build on my Python analysis of NBA statistics that I did in my midterm project, but this time utilizing the predictive models we had learned since then. One of the phenomena in the NBA in recent years has been massive statistical inflation, where the amount of players putting up top scoring numbers has dramatically increased. Where points per game used to be a largely comprehensive metric for looking at who the best NBA players are, it seems that the consensus in recent years has turned to looking at a holistic view of a player, heavily factoring in advanced statistics and team success. To approach this issue, I wanted to see what statistics factor the most into who the best NBA players are. As a proxy for this, I chose All-Star selection, as it combines fan vote, coaches’ opinion, and player choice to choose the best 25-ish players in the league in a given season. The data source I used was the stats section of the official NBA website, which I found easy to work with once it was read in. It has all of the basic per-game counting statistics, as well as shooting stats and their advanced efficiency metric. I also read in historical all star game selections as well as team standings. I chose to train my models on the eight most recent seasons, excluding the 2019-2020 bubble year as it was an outlier season where all teams played different amounts of games, skewing the data. One important caveat of the data is that it uses full season statistics, when all-star selections are made a little past halfway through the season, so if a player finished the year extremely well or poorly, it would be reflected in the training data, but not in their all-star selection. The NBA website does have a statistics filter for segmenting the season to before the all star game, but I was unable to read that data in. Also, I only recognized players as all-stars if they were originally selected to the game, not as an injured player replacement, just to standardize the criteria even more. It seems that every year, there is controversy around the all-star selections based on who the public thought was snubbed from the game or an undeserving selection. I trained three models on 80% of the data from the selected eight seasons, then used these models to predict all star selections for the other 20% of the data. I compared their accuracy, as well as examining who each model thought was snubbed or undeserving. I then used the three models to predict the all-stars for this season based on what has happened so far. Based on the corresponding feature importances and players in each category for each model, it was very interesting to see the consistencies and inconsistencies between what each model values.
	With all three of my models, I added a gridsearch element to hone their performance, scoring on accuracy. All of the models performed fairly well, with accuracy marks clocking above 95%. With this type of data, there isn’t an egregiously bad error that we would want to avoid, like there might be in a model predicting heart cancer. In this case, a type 1 error or false positive would be a player that was predicted to be an all-star but wasn’t selected, making them a “snub” in the model’s eyes. A type 2 error or false negative would be a predicted non-all-star who was selected in actuality, or an “undeserving” selection in the model’s view. In the test data pool of 401 player-seasons, there were 25 all-stars. The decision tree predicted 17 of them, the random forest predicted 19, and the XGBoost model predicted 20 correctly. The XGBoost model also performed the best in terms of minimizing type 2 errors, with only 5 “undeservings” compared to 6 for random forest and 8 for the decision tree. Looking at type 1 errors, the random forest was the best, with only 7 “snubs”, lower than 8 for XGBoost and 11 for the decision tree. All in all, the decision tree model had the lowest accuracy of 95.262%, a bit lower than the random forest and XGBoost models which were tied at 96.758% accuracy. 
	While the random forest and XGBoost models had the same exact accuracy score, the way they weighed different statistics was strikingly dissimilar. Among the 22 statistical categories, the XGBoost model weighed points per game as by far the most important indicator of all-star status, with a near 50% importance mark. The only other feature above 5% was efficiency, which came in at just under 25%. From what I understand about the model, this aligns with how XGBoost prioritizes the most impactful features by iteratively learning from its mistakes, allowing it to zero in on a few key variables. On the other hand, the feature importances of the random forest model were much more evenly distributed, with six statistics above 5% importance. Efficiency and points per game were the most influential, at about 15% and 13%, respectively. Free throws and field goals made were also heavily weighted at around 10% each, in stark contrast to XGBoost, where they were insignificant at less than 2% weight. This broader spread reflects how random forest combines multiple decision trees to provide a more balanced assessment of feature importance. Lastly, the decision tree model exhibited a similar skew to XGBoost, with efficiency accounting for over 50% importance and points per game and win percentage each around 10%. This pattern shows how decision trees tend to focus heavily on the most dominant predictors when breaking the data down into different “branches”. For many of these statistical categories, their contributions overlap. For example, efficiency is all-encompassing, and the shooting stats are broken down into shots made, which also factor into points. For this reason, it’s not too surprising that the models had drastically different feature importances but somewhat similar predictive results and accuracy.
	Taking a look at the players that the models saw as snubs and undeserving further gave a view into the type of players that each model prioritized, and also perhaps when the all-star vote was “wrong”. All three models agreed that Bam Adebayo in 2021, James Harden in 2023 and 2024, Zach Lavine in 2023, Devin Booker in 2021. and Paul George in 2018 deserved to make the all star game. Booker’s 2021 snub seems to be the most glaring, as 25 points per game on a 2 seed is a near-MVP level season but speaks to the point inflation and increasing level of talent in the modern NBA. The decision tree’s extreme prioritization of the efficiency stat showed, as it chose Hassan Whiteside’s 2016 and Pascal Siakam’s 2024 seasons as deserving of all-star bids. While their stats for the most part were pretty pedestrian, they both did put up efficiency numbers on par with that of current superstars Anthony Edwards and Devin Booker. Having more than half of the weight be on a single stat may not be advantageous, as players such as Whiteside and Siakam who excel in it are darlings of the decision tree but not of the other models or public opinion for that matter.
	Taking a look at the players the models chose as undeserving, they agreed on 2017 Kemba Walker, 2022 Fred Vanvleet, and 2022 Andrew Wiggins as their consensus undeserving selections. The Wiggins selection highlights a shortcoming of the dataset, as he was voted a starter by the fans. Popularity of players and teams was not taken into account here, so leads to a blindspot in the predictive ability. In the future, it is very likely that players such as Lebron James and Steph Curry will continue to be voted in by the fans, even if their stats and team record are subpar. The players that the random forest model saw as undeserving such as 2021 Ben Simmons and 2017 Draymond Green highlight how though the model has the most even distribution of feature importances, 4 of the top 5 in terms of weight have to do with taking a large quantity of shots. This is not part of Green and Simmons’ play styles, even though they did excel in other things in their all-star seasons.
	The part I was most looking forward to about this project was the predictive capability of these models, and I’m pretty happy with how they turned out. So far this season, all three models agree that Giannis Antetokounmpo, Nikola Jokic, Lamelo Ball, Shai Gilgeous-Alexander, Luka Doncic, Jayson Tatum, Anthony Davis, Anthony Edwards, De’Aaron Fox, Damian Lillard, Jalen Brunson, Karl-Anthony Towns, Tyler Herro, Kyrie Irving, Jaylen Brown, Donovan Mitchell,  Lebron James, Jaren Jackson Jr., Jalen Williams, James Harden, and Alperen Sengun should make this year’s all-star game. In terms of players not unanimously agreed on by the models, the decision tree skipped on young stars Cade Cunningham and Victor Wembanyama, players who were selected by the other two models as deserving. Cunningham and Wembanyama star on the 11 seeds in their respective conferences, and the decision tree left them off in favor of supporting players on winning teams such as Derrick White and Darius Garland. This lines up with the decision tree model being the one that gave the most weight to win percentage, weighing it more than double the amount the other models did. The XGBoost model uniquely selected Trae Young, Evan Mobley, and perhaps most surprisingly Josh Hart to be all stars while the other two models left these players off. I see no distinct single statistical measure of these players that features prominently in the importance weight of the XGBoost model as compared to the other two, and the three players have somewhat different archetypes, so this is very interesting. 
	Looking back at the models, the decision tree model was the worst, with it relying far too heavily on one feature and having the lowest accuracy score of the three models. In addition, it didn’t pass the basketball fan smell test, as it touted some players as all-star game worthy that probably weren’t there for a reason. Though the random forest and the XGBoost models had the same accuracy score, I prefer the random forest, as it has a much more even distribution of the weight of its feature importances. In addition, I feel that its all-star predictions for this season were the most realistic, even though I am partial to the XGBoost’s selection of three Cleveland Cavaliers. Another key takeaway I had was that though I went in wanting to see a more holistic focus on stats as evaluating player performance beyond points per game, but it seems points per game is the most effective of the counting stats, being one of the top two in weight for all three models. In fact, all three models had the same top two feature importances: points and efficiency. When looking for a quick indicator on how good a player is, it seems that these statistics can tell you the most out of the ones I looked at in my data. For further analysis, I might want to incorporate a time series modeling element to predict the arc of a players career, though this would probably include things beyond the scope of the course. This type of analysis when done right could have all sorts of implications. Some that come to mind are as tools for a front office to evaluate a player’s quality, a possible edge for gamblers looking to predict NBA futures, or leverage for front offices or scouts seeking advantageous terms on their players’ contracts. Looking back on my simpler non-time-series work, it could have use cases in creating new advanced stats as well as assigning things like ranking or rating in the 2K video game. I look forward to updating the file as the season progresses and seeing how close each model gets at predicting the real all-star selections. Looking back over this project, I never would’ve expected myself to learn how to do something like this over the course of this semester, and as a huge sports analytics guy, I’m excited to use these tools in more applications. 


